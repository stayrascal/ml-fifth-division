{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_file = '../data/heart.csv'\n",
    "origin_data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 9) (277, 2)\n"
     ]
    }
   ],
   "source": [
    "def load_data(data = origin_data, stragy = 2):\n",
    "    positive_example = data[data['chd'] == 1]\n",
    "    negitive_example = data[data['chd'] == 0]\n",
    "\n",
    "    if stragy == 1:\n",
    "        positive_example = pd.concat([positive_example, positive_example])\n",
    "    elif stragy == 2: \n",
    "        negitive_example = data[data['chd'] == 0]\n",
    "        negitive_index = random.sample(list(negitive_example.index.values), len(positive_example))\n",
    "        negitive_example = negitive_example.ix[negitive_index]\n",
    "\n",
    "    positive_msk = np.random.rand(len(positive_example)) < 0.9\n",
    "    negitive_msk = np.random.rand(len(negitive_example)) < 0.9\n",
    "    \n",
    "    while np.abs(len(positive_example[positive_msk]) - len(negitive_example[negitive_msk])) > 10:\n",
    "        positive_msk = np.random.rand(len(positive_example)) < 0.9\n",
    "        negitive_msk = np.random.rand(len(negitive_example)) < 0.9\n",
    "    \n",
    "    train_dataset = pd.concat([positive_example[positive_msk], negitive_example[negitive_msk]])\n",
    "    test_dataset = pd.concat([positive_example[~positive_msk], negitive_example[~negitive_msk]])\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def normaliztion(dataset):\n",
    "    return dataset.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "\n",
    "def transfer_famhist(dataset):\n",
    "    tmp = dataset.replace({'famhist':{'Present':1, 'Absent':0}})\n",
    "    return normaliztion(tmp).values\n",
    "\n",
    "def to_one_hotting(data, num_lables=2):\n",
    "    return (np.arange(num_lables) == data[:,None]).astype(np.float32)\n",
    "\n",
    "def generate_data(dataset):\n",
    "    data = dataset.iloc[:,0:9]\n",
    "    labels = dataset['chd']\n",
    "    return randomize(transfer_famhist(data), to_one_hotting(labels.values))\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, test_dataset = load_data(origin_data)\n",
    "\n",
    "train_data, train_label = generate_data(train_dataset)\n",
    "test_data, test_label = generate_data(test_dataset)\n",
    "\n",
    "print(train_data.shape, train_label.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 16\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch :0.8555820408989402\n",
      "Average loss epoch :0.7877266231705161\n",
      "Average loss epoch :0.7354915387490216\n",
      "Average loss epoch :0.6948846736375023\n",
      "Average loss epoch :0.6630223375909469\n",
      "Average loss epoch :0.6376643461339614\n",
      "Average loss epoch :0.6172758586266461\n",
      "Average loss epoch :0.6008069427574382\n",
      "Average loss epoch :0.5874951201326707\n",
      "Average loss epoch :0.5767462130855111\n",
      "Average loss epoch :0.5680746050441966\n",
      "Average loss epoch :0.5610761046409607\n",
      "Average loss epoch :0.5554152779719409\n",
      "Average loss epoch :0.5508173511308783\n",
      "Average loss epoch :0.5470607385915869\n",
      "Average loss epoch :0.5439693471964668\n",
      "Average loss epoch :0.5414047328864827\n",
      "Average loss epoch :0.5392589078230017\n",
      "Average loss epoch :0.5374479206169352\n",
      "Average loss epoch :0.535906533984577\n",
      "Average loss epoch :0.5345840068424449\n",
      "Average loss epoch :0.5334406495094299\n",
      "Average loss epoch :0.5324453115463257\n",
      "Average loss epoch :0.5315733324078953\n",
      "Average loss epoch :0.5308050039936515\n",
      "Average loss epoch :0.5301245240604177\n",
      "Average loss epoch :0.5295190635849448\n",
      "Average loss epoch :0.5289781216312858\n",
      "Average loss epoch :0.5284929854028365\n",
      "Average loss epoch :0.5280564883176018\n",
      "Average loss epoch :0.5276625366771922\n",
      "Average loss epoch :0.5273059922106126\n",
      "Average loss epoch :0.5269825107911054\n",
      "Average loss epoch :0.5266883092768052\n",
      "Average loss epoch :0.5264201725230497\n",
      "Average loss epoch :0.526175304370768\n",
      "Average loss epoch :0.5259512329802793\n",
      "Average loss epoch :0.5257458160905277\n",
      "Average loss epoch :0.5255571656367358\n",
      "Average loss epoch :0.5253836354788612\n",
      "Average loss epoch :0.5252237284884733\n",
      "Average loss epoch :0.5250761316103094\n",
      "Average loss epoch :0.524939708849963\n",
      "Average loss epoch :0.5248133908299839\n",
      "Average loss epoch :0.5246962554314557\n",
      "Average loss epoch :0.5245874699424294\n",
      "Average loss epoch :0.5244862840456121\n",
      "Average loss epoch :0.5243920403368333\n",
      "Average loss epoch :0.5243041234857896\n",
      "Average loss epoch :0.5242219952976003\n",
      "Average loss epoch :0.5241451578981736\n",
      "Average loss epoch :0.5240731747711406\n",
      "Average loss epoch :0.5240056409555323\n",
      "Average loss epoch :0.5239421760334688\n",
      "Average loss epoch :0.5238824732163373\n",
      "Average loss epoch :0.5238262502586141\n",
      "Average loss epoch :0.5237732073839974\n",
      "Average loss epoch :0.5237230956554413\n",
      "Average loss epoch :0.5236757222343894\n",
      "Average loss epoch :0.523630852208418\n",
      "Average loss epoch :0.5235883120228263\n",
      "Average loss epoch :0.5235479403944576\n",
      "Average loss epoch :0.5235095795463113\n",
      "Average loss epoch :0.5234730717013863\n",
      "Average loss epoch :0.5234383134280934\n",
      "Average loss epoch :0.5234051609740538\n",
      "Average loss epoch :0.523373535450767\n",
      "Average loss epoch :0.5233433106366325\n",
      "Average loss epoch :0.5233144269270056\n",
      "Average loss epoch :0.5232867633595186\n",
      "Average loss epoch :0.5232602620826048\n",
      "Average loss epoch :0.5232348512200748\n",
      "Average loss epoch :0.5232104694142061\n",
      "Average loss epoch :0.5231870605665094\n",
      "Average loss epoch :0.5231645615661845\n",
      "Average loss epoch :0.5231428917716531\n",
      "Average loss epoch :0.5231220792321598\n",
      "Average loss epoch :0.5231020064914927\n",
      "Average loss epoch :0.5230826612781075\n",
      "Average loss epoch :0.5230640067773706\n",
      "Average loss epoch :0.5230460114338819\n",
      "Average loss epoch :0.5230286401860854\n",
      "Average loss epoch :0.5230118457008811\n",
      "Average loss epoch :0.5229956209659576\n",
      "Average loss epoch :0.5229799449443817\n",
      "Average loss epoch :0.5229647597845863\n",
      "Average loss epoch :0.5229500882765826\n",
      "Average loss epoch :0.5229358708157259\n",
      "Average loss epoch :0.5229220968835494\n",
      "Average loss epoch :0.5229087471961975\n",
      "Average loss epoch :0.5228958112352035\n",
      "Average loss epoch :0.5228832732228672\n",
      "Average loss epoch :0.5228711173814886\n",
      "Average loss epoch :0.5228593121556675\n",
      "Average loss epoch :0.5228478628046372\n",
      "Average loss epoch :0.5228367570568534\n",
      "Average loss epoch :0.5228259633569157\n",
      "Average loss epoch :0.5228154799517464\n",
      "Average loss epoch :0.5228052980759564\n",
      "Average loss epoch :0.5227954072110793\n",
      "Total time: 0.7456309795379639 seconds\n",
      "Optimization Finished!\n",
      "Accuracy: 0.4883720930232558\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(dtype = np.float32, shape = [batch_size, 9], name='X')\n",
    "Y = tf.placeholder(dtype = np.float32, shape = [batch_size, 2], name='Y')\n",
    "\n",
    "W = tf.Variable(tf.random_normal([9, 2]), name='W')\n",
    "b = tf.Variable(tf.random_normal([batch_size, 2]), name='b')\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    n_batches = int(len(train_data)/batch_size)\n",
    "    for i in range(n_epochs): \n",
    "        total_loss = 0\n",
    "\n",
    "        for index in range(n_batches):\n",
    "            \n",
    "            X_batch = train_data[index*batch_size:(index+1)*batch_size]\n",
    "            Y_batch = train_label[index*batch_size:(index+1)*batch_size]\n",
    "            \n",
    "            _, loss_batch= sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_loss += loss_batch\n",
    "            \n",
    "        test_accuracy = sess.run([accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "        print('Average loss epoch :{0}'.format(total_loss/n_batches))\n",
    "        \n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!')\n",
    "\n",
    "    # test the model\n",
    "    n_batches = int(len(test_data)/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for index in range(n_batches):\n",
    "        X_batch = test_data[index*batch_size:(index+1)*batch_size]\n",
    "        Y_batch = test_label[index*batch_size:(index+1)*batch_size]\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "\n",
    "    print('Accuracy:',format(total_correct_preds/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
